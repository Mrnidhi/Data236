Loading embedding model...
Loading weights:   0%|          | 0/103 [00:00<?, ?it/s]Loading weights:   1%|          | 1/103 [00:00<00:00, 5461.33it/s, Materializing param=embeddings.LayerNorm.bias]Loading weights:   1%|          | 1/103 [00:00<00:00, 3816.47it/s, Materializing param=embeddings.LayerNorm.bias]Loading weights:   2%|â–         | 2/103 [00:00<00:00, 3396.20it/s, Materializing param=embeddings.LayerNorm.weight]Loading weights:   2%|â–         | 2/103 [00:00<00:00, 2047.00it/s, Materializing param=embeddings.LayerNorm.weight]Loading weights:   3%|â–Ž         | 3/103 [00:00<00:00, 2860.40it/s, Materializing param=embeddings.position_embeddings.weight]Loading weights:   3%|â–Ž         | 3/103 [00:00<00:00, 2772.79it/s, Materializing param=embeddings.position_embeddings.weight]Loading weights:   4%|â–         | 4/103 [00:00<00:00, 2262.30it/s, Materializing param=embeddings.token_type_embeddings.weight]Loading weights:   4%|â–         | 4/103 [00:00<00:00, 2211.31it/s, Materializing param=embeddings.token_type_embeddings.weight]Loading weights:   5%|â–         | 5/103 [00:00<00:00, 2685.21it/s, Materializing param=embeddings.word_embeddings.weight]      Loading weights:   5%|â–         | 5/103 [00:00<00:00, 2650.93it/s, Materializing param=embeddings.word_embeddings.weight]Loading weights:   6%|â–Œ         | 6/103 [00:00<00:00, 2682.07it/s, Materializing param=encoder.layer.0.attention.output.LayerNorm.bias]Loading weights:   6%|â–Œ         | 6/103 [00:00<00:00, 2598.70it/s, Materializing param=encoder.layer.0.attention.output.LayerNorm.bias]Loading weights:   7%|â–‹         | 7/103 [00:00<00:00, 2698.29it/s, Materializing param=encoder.layer.0.attention.output.LayerNorm.weight]Loading weights:   7%|â–‹         | 7/103 [00:00<00:00, 2599.39it/s, Materializing param=encoder.layer.0.attention.output.LayerNorm.weight]Loading weights:   8%|â–Š         | 8/103 [00:00<00:00, 2860.32it/s, Materializing param=encoder.layer.0.attention.output.dense.bias]      Loading weights:   8%|â–Š         | 8/103 [00:00<00:00, 2831.84it/s, Materializing param=encoder.layer.0.attention.output.dense.bias]Loading weights:   9%|â–Š         | 9/103 [00:00<00:00, 2964.41it/s, Materializing param=encoder.layer.0.attention.output.dense.weight]Loading weights:   9%|â–Š         | 9/103 [00:00<00:00, 2938.10it/s, Materializing param=encoder.layer.0.attention.output.dense.weight]Loading weights:  10%|â–‰         | 10/103 [00:00<00:00, 2869.47it/s, Materializing param=encoder.layer.0.attention.self.key.bias]     Loading weights:  10%|â–‰         | 10/103 [00:00<00:00, 2818.56it/s, Materializing param=encoder.layer.0.attention.self.key.bias]Loading weights:  11%|â–ˆ         | 11/103 [00:00<00:00, 3044.57it/s, Materializing param=encoder.layer.0.attention.self.key.weight]Loading weights:  11%|â–ˆ         | 11/103 [00:00<00:00, 2885.75it/s, Materializing param=encoder.layer.0.attention.self.key.weight]Loading weights:  12%|â–ˆâ–        | 12/103 [00:00<00:00, 2507.31it/s, Materializing param=encoder.layer.0.attention.self.query.bias]Loading weights:  12%|â–ˆâ–        | 12/103 [00:00<00:00, 2492.78it/s, Materializing param=encoder.layer.0.attention.self.query.bias]Loading weights:  13%|â–ˆâ–Ž        | 13/103 [00:00<00:00, 2005.88it/s, Materializing param=encoder.layer.0.attention.self.query.weight]Loading weights:  13%|â–ˆâ–Ž        | 13/103 [00:00<00:00, 1802.09it/s, Materializing param=encoder.layer.0.attention.self.query.weight]Loading weights:  14%|â–ˆâ–Ž        | 14/103 [00:00<00:00, 1894.69it/s, Materializing param=encoder.layer.0.attention.self.value.bias]  Loading weights:  14%|â–ˆâ–Ž        | 14/103 [00:00<00:00, 1886.53it/s, Materializing param=encoder.layer.0.attention.self.value.bias]Loading weights:  15%|â–ˆâ–        | 15/103 [00:00<00:00, 2005.56it/s, Materializing param=encoder.layer.0.attention.self.value.weight]Loading weights:  15%|â–ˆâ–        | 15/103 [00:00<00:00, 1999.19it/s, Materializing param=encoder.layer.0.attention.self.value.weight]Loading weights:  16%|â–ˆâ–Œ        | 16/103 [00:00<00:00, 2119.74it/s, Materializing param=encoder.layer.0.intermediate.dense.bias]    Loading weights:  16%|â–ˆâ–Œ        | 16/103 [00:00<00:00, 2112.73it/s, Materializing param=encoder.layer.0.intermediate.dense.bias]Loading weights:  17%|â–ˆâ–‹        | 17/103 [00:00<00:00, 2231.57it/s, Materializing param=encoder.layer.0.intermediate.dense.weight]Loading weights:  17%|â–ˆâ–‹        | 17/103 [00:00<00:00, 2224.89it/s, Materializing param=encoder.layer.0.intermediate.dense.weight]Loading weights:  17%|â–ˆâ–‹        | 18/103 [00:00<00:00, 2267.06it/s, Materializing param=encoder.layer.0.output.LayerNorm.bias]    Loading weights:  17%|â–ˆâ–‹        | 18/103 [00:00<00:00, 2258.71it/s, Materializing param=encoder.layer.0.output.LayerNorm.bias]Loading weights:  18%|â–ˆâ–Š        | 19/103 [00:00<00:00, 2365.23it/s, Materializing param=encoder.layer.0.output.LayerNorm.weight]Loading weights:  18%|â–ˆâ–Š        | 19/103 [00:00<00:00, 2346.84it/s, Materializing param=encoder.layer.0.output.LayerNorm.weight]Loading weights:  19%|â–ˆâ–‰        | 20/103 [00:00<00:00, 2452.16it/s, Materializing param=encoder.layer.0.output.dense.bias]      Loading weights:  19%|â–ˆâ–‰        | 20/103 [00:00<00:00, 2444.66it/s, Materializing param=encoder.layer.0.output.dense.bias]Loading weights:  20%|â–ˆâ–ˆ        | 21/103 [00:00<00:00, 1780.88it/s, Materializing param=encoder.layer.0.output.dense.weight]Loading weights:  20%|â–ˆâ–ˆ        | 21/103 [00:00<00:00, 1765.42it/s, Materializing param=encoder.layer.0.output.dense.weight]Loading weights:  21%|â–ˆâ–ˆâ–       | 22/103 [00:00<00:00, 1829.79it/s, Materializing param=encoder.layer.1.attention.output.LayerNorm.bias]Loading weights:  21%|â–ˆâ–ˆâ–       | 22/103 [00:00<00:00, 1822.57it/s, Materializing param=encoder.layer.1.attention.output.LayerNorm.bias]Loading weights:  22%|â–ˆâ–ˆâ–       | 23/103 [00:00<00:00, 1880.93it/s, Materializing param=encoder.layer.1.attention.output.LayerNorm.weight]Loading weights:  22%|â–ˆâ–ˆâ–       | 23/103 [00:00<00:00, 1874.68it/s, Materializing param=encoder.layer.1.attention.output.LayerNorm.weight]Loading weights:  23%|â–ˆâ–ˆâ–Ž       | 24/103 [00:00<00:00, 1939.86it/s, Materializing param=encoder.layer.1.attention.output.dense.bias]      Loading weights:  23%|â–ˆâ–ˆâ–Ž       | 24/103 [00:00<00:00, 1933.45it/s, Materializing param=encoder.layer.1.attention.output.dense.bias]Loading weights:  24%|â–ˆâ–ˆâ–       | 25/103 [00:00<00:00, 2004.35it/s, Materializing param=encoder.layer.1.attention.output.dense.weight]Loading weights:  24%|â–ˆâ–ˆâ–       | 25/103 [00:00<00:00, 2000.49it/s, Materializing param=encoder.layer.1.attention.output.dense.weight]Loading weights:  25%|â–ˆâ–ˆâ–Œ       | 26/103 [00:00<00:00, 2065.30it/s, Materializing param=encoder.layer.1.attention.self.key.bias]      Loading weights:  25%|â–ˆâ–ˆâ–Œ       | 26/103 [00:00<00:00, 2052.24it/s, Materializing param=encoder.layer.1.attention.self.key.bias]Loading weights:  26%|â–ˆâ–ˆâ–Œ       | 27/103 [00:00<00:00, 2109.54it/s, Materializing param=encoder.layer.1.attention.self.key.weight]Loading weights:  26%|â–ˆâ–ˆâ–Œ       | 27/103 [00:00<00:00, 2105.26it/s, Materializing param=encoder.layer.1.attention.self.key.weight]Loading weights:  27%|â–ˆâ–ˆâ–‹       | 28/103 [00:00<00:00, 2171.05it/s, Materializing param=encoder.layer.1.attention.self.query.bias]Loading weights:  27%|â–ˆâ–ˆâ–‹       | 28/103 [00:00<00:00, 2080.07it/s, Materializing param=encoder.layer.1.attention.self.query.bias]Loading weights:  28%|â–ˆâ–ˆâ–Š       | 29/103 [00:00<00:00, 2130.95it/s, Materializing param=encoder.layer.1.attention.self.query.weight]Loading weights:  28%|â–ˆâ–ˆâ–Š       | 29/103 [00:00<00:00, 2125.93it/s, Materializing param=encoder.layer.1.attention.self.query.weight]Loading weights:  29%|â–ˆâ–ˆâ–‰       | 30/103 [00:00<00:00, 2190.12it/s, Materializing param=encoder.layer.1.attention.self.value.bias]  Loading weights:  29%|â–ˆâ–ˆâ–‰       | 30/103 [00:00<00:00, 2184.38it/s, Materializing param=encoder.layer.1.attention.self.value.bias]Loading weights:  30%|â–ˆâ–ˆâ–ˆ       | 31/103 [00:00<00:00, 2185.41it/s, Materializing param=encoder.layer.1.attention.self.value.weight]Loading weights:  30%|â–ˆâ–ˆâ–ˆ       | 31/103 [00:00<00:00, 2180.94it/s, Materializing param=encoder.layer.1.attention.self.value.weight]Loading weights:  31%|â–ˆâ–ˆâ–ˆ       | 32/103 [00:00<00:00, 2242.61it/s, Materializing param=encoder.layer.1.intermediate.dense.bias]    Loading weights:  31%|â–ˆâ–ˆâ–ˆ       | 32/103 [00:00<00:00, 2209.96it/s, Materializing param=encoder.layer.1.intermediate.dense.bias]Loading weights:  32%|â–ˆâ–ˆâ–ˆâ–      | 33/103 [00:00<00:00, 2268.34it/s, Materializing param=encoder.layer.1.intermediate.dense.weight]Loading weights:  32%|â–ˆâ–ˆâ–ˆâ–      | 33/103 [00:00<00:00, 2264.34it/s, Materializing param=encoder.layer.1.intermediate.dense.weight]Loading weights:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 34/103 [00:00<00:00, 2322.09it/s, Materializing param=encoder.layer.1.output.LayerNorm.bias]    Loading weights:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 34/103 [00:00<00:00, 2314.21it/s, Materializing param=encoder.layer.1.output.LayerNorm.bias]Loading weights:  34%|â–ˆâ–ˆâ–ˆâ–      | 35/103 [00:00<00:00, 2371.77it/s, Materializing param=encoder.layer.1.output.LayerNorm.weight]Loading weights:  34%|â–ˆâ–ˆâ–ˆâ–      | 35/103 [00:00<00:00, 2358.51it/s, Materializing param=encoder.layer.1.output.LayerNorm.weight]Loading weights:  35%|â–ˆâ–ˆâ–ˆâ–      | 36/103 [00:00<00:00, 2411.75it/s, Materializing param=encoder.layer.1.output.dense.bias]      Loading weights:  35%|â–ˆâ–ˆâ–ˆâ–      | 36/103 [00:00<00:00, 2406.60it/s, Materializing param=encoder.layer.1.output.dense.bias]Loading weights:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 37/103 [00:00<00:00, 2453.74it/s, Materializing param=encoder.layer.1.output.dense.weight]Loading weights:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 37/103 [00:00<00:00, 2447.90it/s, Materializing param=encoder.layer.1.output.dense.weight]Loading weights:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 38/103 [00:00<00:00, 2492.94it/s, Materializing param=encoder.layer.2.attention.output.LayerNorm.bias]Loading weights:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 38/103 [00:00<00:00, 2487.72it/s, Materializing param=encoder.layer.2.attention.output.LayerNorm.bias]Loading weights:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 39/103 [00:00<00:00, 2535.42it/s, Materializing param=encoder.layer.2.attention.output.LayerNorm.weight]Loading weights:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 39/103 [00:00<00:00, 2518.56it/s, Materializing param=encoder.layer.2.attention.output.LayerNorm.weight]Loading weights:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 40/103 [00:00<00:00, 2485.40it/s, Materializing param=encoder.layer.2.attention.output.dense.bias]      Loading weights:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 40/103 [00:00<00:00, 2480.63it/s, Materializing param=encoder.layer.2.attention.output.dense.bias]Loading weights:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 41/103 [00:00<00:00, 2533.24it/s, Materializing param=encoder.layer.2.attention.output.dense.weight]Loading weights:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 41/103 [00:00<00:00, 2529.77it/s, Materializing param=encoder.layer.2.attention.output.dense.weight]Loading weights:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 42/103 [00:00<00:00, 2585.24it/s, Materializing param=encoder.layer.2.attention.self.key.bias]      Loading weights:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 42/103 [00:00<00:00, 2581.45it/s, Materializing param=encoder.layer.2.attention.self.key.bias]Loading weights:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 43/103 [00:00<00:00, 2629.96it/s, Materializing param=encoder.layer.2.attention.self.key.weight]Loading weights:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 43/103 [00:00<00:00, 2614.79it/s, Materializing param=encoder.layer.2.attention.self.key.weight]Loading weights:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 44/103 [00:00<00:00, 2667.48it/s, Materializing param=encoder.layer.2.attention.self.query.bias]Loading weights:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 44/103 [00:00<00:00, 2662.67it/s, Materializing param=encoder.layer.2.attention.self.query.bias]Loading weights:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 45/103 [00:00<00:00, 2704.99it/s, Materializing param=encoder.layer.2.attention.self.query.weight]Loading weights:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 45/103 [00:00<00:00, 2700.27it/s, Materializing param=encoder.layer.2.attention.self.query.weight]Loading weights:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 46/103 [00:00<00:00, 2747.43it/s, Materializing param=encoder.layer.2.attention.self.value.bias]  Loading weights:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 46/103 [00:00<00:00, 2742.86it/s, Materializing param=encoder.layer.2.attention.self.value.bias]Loading weights:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 47/103 [00:00<00:00, 2781.72it/s, Materializing param=encoder.layer.2.attention.self.value.weight]Loading weights:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 47/103 [00:00<00:00, 2777.92it/s, Materializing param=encoder.layer.2.attention.self.value.weight]Loading weights:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 48/103 [00:00<00:00, 2828.06it/s, Materializing param=encoder.layer.2.intermediate.dense.bias]    Loading weights:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 48/103 [00:00<00:00, 2821.20it/s, Materializing param=encoder.layer.2.intermediate.dense.bias]Loading weights:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 49/103 [00:00<00:00, 2873.05it/s, Materializing param=encoder.layer.2.intermediate.dense.weight]Loading weights:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 49/103 [00:00<00:00, 2869.16it/s, Materializing param=encoder.layer.2.intermediate.dense.weight]Loading weights:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 50/103 [00:00<00:00, 2920.38it/s, Materializing param=encoder.layer.2.output.LayerNorm.bias]    Loading weights:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 50/103 [00:00<00:00, 2916.32it/s, Materializing param=encoder.layer.2.output.LayerNorm.bias]Loading weights:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 51/103 [00:00<00:00, 2965.78it/s, Materializing param=encoder.layer.2.output.LayerNorm.weight]Loading weights:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 51/103 [00:00<00:00, 2961.67it/s, Materializing param=encoder.layer.2.output.LayerNorm.weight]Loading weights:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 52/103 [00:00<00:00, 3012.90it/s, Materializing param=encoder.layer.2.output.dense.bias]      Loading weights:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 52/103 [00:00<00:00, 3006.30it/s, Materializing param=encoder.layer.2.output.dense.bias]Loading weights:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 53/103 [00:00<00:00, 3054.93it/s, Materializing param=encoder.layer.2.output.dense.weight]Loading weights:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 53/103 [00:00<00:00, 3050.91it/s, Materializing param=encoder.layer.2.output.dense.weight]Loading weights:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 54/103 [00:00<00:00, 3100.77it/s, Materializing param=encoder.layer.3.attention.output.LayerNorm.bias]Loading weights:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 54/103 [00:00<00:00, 3096.70it/s, Materializing param=encoder.layer.3.attention.output.LayerNorm.bias]Loading weights:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 55/103 [00:00<00:00, 3144.11it/s, Materializing param=encoder.layer.3.attention.output.LayerNorm.weight]Loading weights:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 55/103 [00:00<00:00, 3139.96it/s, Materializing param=encoder.layer.3.attention.output.LayerNorm.weight]Loading weights:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 56/103 [00:00<00:00, 3189.80it/s, Materializing param=encoder.layer.3.attention.output.dense.bias]      Loading weights:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 56/103 [00:00<00:00, 3185.78it/s, Materializing param=encoder.layer.3.attention.output.dense.bias]Loading weights:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 57/103 [00:00<00:00, 3233.33it/s, Materializing param=encoder.layer.3.attention.output.dense.weight]Loading weights:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 57/103 [00:00<00:00, 3228.22it/s, Materializing param=encoder.layer.3.attention.output.dense.weight]Loading weights:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 58/103 [00:00<00:00, 3276.84it/s, Materializing param=encoder.layer.3.attention.self.key.bias]      Loading weights:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 58/103 [00:00<00:00, 3272.57it/s, Materializing param=encoder.layer.3.attention.self.key.bias]Loading weights:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 59/103 [00:00<00:00, 3318.90it/s, Materializing param=encoder.layer.3.attention.self.key.weight]Loading weights:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 59/103 [00:00<00:00, 3314.23it/s, Materializing param=encoder.layer.3.attention.self.key.weight]Loading weights:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 60/103 [00:00<00:00, 3360.96it/s, Materializing param=encoder.layer.3.attention.self.query.bias]Loading weights:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 60/103 [00:00<00:00, 3357.01it/s, Materializing param=encoder.layer.3.attention.self.query.bias]Loading weights:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 61/103 [00:00<00:00, 3404.60it/s, Materializing param=encoder.layer.3.attention.self.query.weight]Loading weights:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 61/103 [00:00<00:00, 3400.40it/s, Materializing param=encoder.layer.3.attention.self.query.weight]Loading weights:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 62/103 [00:00<00:00, 3449.08it/s, Materializing param=encoder.layer.3.attention.self.value.bias]  Loading weights:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 62/103 [00:00<00:00, 3445.20it/s, Materializing param=encoder.layer.3.attention.self.value.bias]Loading weights:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 63/103 [00:00<00:00, 3494.01it/s, Materializing param=encoder.layer.3.attention.self.value.weight]Loading weights:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 63/103 [00:00<00:00, 3488.56it/s, Materializing param=encoder.layer.3.attention.self.value.weight]Loading weights:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 64/103 [00:00<00:00, 3535.30it/s, Materializing param=encoder.layer.3.intermediate.dense.bias]    Loading weights:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 64/103 [00:00<00:00, 3531.39it/s, Materializing param=encoder.layer.3.intermediate.dense.bias]Loading weights:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 65/103 [00:00<00:00, 3576.93it/s, Materializing param=encoder.layer.3.intermediate.dense.weight]Loading weights:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 65/103 [00:00<00:00, 3572.01it/s, Materializing param=encoder.layer.3.intermediate.dense.weight]Loading weights:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 66/103 [00:00<00:00, 3619.23it/s, Materializing param=encoder.layer.3.output.LayerNorm.bias]    Loading weights:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 66/103 [00:00<00:00, 3615.07it/s, Materializing param=encoder.layer.3.output.LayerNorm.bias]Loading weights:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 67/103 [00:00<00:00, 3662.19it/s, Materializing param=encoder.layer.3.output.LayerNorm.weight]Loading weights:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 67/103 [00:00<00:00, 3658.19it/s, Materializing param=encoder.layer.3.output.LayerNorm.weight]Loading weights:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 68/103 [00:00<00:00, 3705.94it/s, Materializing param=encoder.layer.3.output.dense.bias]      Loading weights:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 68/103 [00:00<00:00, 3702.09it/s, Materializing param=encoder.layer.3.output.dense.bias]Loading weights:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 69/103 [00:00<00:00, 3749.57it/s, Materializing param=encoder.layer.3.output.dense.weight]Loading weights:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 69/103 [00:00<00:00, 3745.69it/s, Materializing param=encoder.layer.3.output.dense.weight]Loading weights:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 70/103 [00:00<00:00, 3793.20it/s, Materializing param=encoder.layer.4.attention.output.LayerNorm.bias]Loading weights:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 70/103 [00:00<00:00, 3789.09it/s, Materializing param=encoder.layer.4.attention.output.LayerNorm.bias]Loading weights:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 71/103 [00:00<00:00, 3835.35it/s, Materializing param=encoder.layer.4.attention.output.LayerNorm.weight]Loading weights:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 71/103 [00:00<00:00, 3831.01it/s, Materializing param=encoder.layer.4.attention.output.LayerNorm.weight]Loading weights:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 72/103 [00:00<00:00, 3877.63it/s, Materializing param=encoder.layer.4.attention.output.dense.bias]      Loading weights:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 72/103 [00:00<00:00, 3871.37it/s, Materializing param=encoder.layer.4.attention.output.dense.bias]Loading weights:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 73/103 [00:00<00:00, 3916.10it/s, Materializing param=encoder.layer.4.attention.output.dense.weight]Loading weights:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 73/103 [00:00<00:00, 3910.40it/s, Materializing param=encoder.layer.4.attention.output.dense.weight]Loading weights:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 74/103 [00:00<00:00, 3955.08it/s, Materializing param=encoder.layer.4.attention.self.key.bias]      Loading weights:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 74/103 [00:00<00:00, 3950.65it/s, Materializing param=encoder.layer.4.attention.self.key.bias]Loading weights:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 75/103 [00:00<00:00, 3995.74it/s, Materializing param=encoder.layer.4.attention.self.key.weight]Loading weights:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 75/103 [00:00<00:00, 3989.81it/s, Materializing param=encoder.layer.4.attention.self.key.weight]Loading weights:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 76/103 [00:00<00:00, 4031.81it/s, Materializing param=encoder.layer.4.attention.self.query.bias]Loading weights:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 76/103 [00:00<00:00, 4026.72it/s, Materializing param=encoder.layer.4.attention.self.query.bias]Loading weights:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 77/103 [00:00<00:00, 4068.70it/s, Materializing param=encoder.layer.4.attention.self.query.weight]Loading weights:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 77/103 [00:00<00:00, 4064.20it/s, Materializing param=encoder.layer.4.attention.self.query.weight]Loading weights:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 78/103 [00:00<00:00, 4107.67it/s, Materializing param=encoder.layer.4.attention.self.value.bias]  Loading weights:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 78/103 [00:00<00:00, 4102.68it/s, Materializing param=encoder.layer.4.attention.self.value.bias]Loading weights:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 79/103 [00:00<00:00, 4143.95it/s, Materializing param=encoder.layer.4.attention.self.value.weight]Loading weights:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 79/103 [00:00<00:00, 4139.39it/s, Materializing param=encoder.layer.4.attention.self.value.weight]Loading weights:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 80/103 [00:00<00:00, 4183.01it/s, Materializing param=encoder.layer.4.intermediate.dense.bias]    Loading weights:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 80/103 [00:00<00:00, 4176.24it/s, Materializing param=encoder.layer.4.intermediate.dense.bias]Loading weights:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 81/103 [00:00<00:00, 4215.01it/s, Materializing param=encoder.layer.4.intermediate.dense.weight]Loading weights:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 81/103 [00:00<00:00, 4210.42it/s, Materializing param=encoder.layer.4.intermediate.dense.weight]Loading weights:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 82/103 [00:00<00:00, 4253.81it/s, Materializing param=encoder.layer.4.output.LayerNorm.bias]    Loading weights:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 82/103 [00:00<00:00, 4248.71it/s, Materializing param=encoder.layer.4.output.LayerNorm.bias]Loading weights:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 83/103 [00:00<00:00, 4288.76it/s, Materializing param=encoder.layer.4.output.LayerNorm.weight]Loading weights:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 83/103 [00:00<00:00, 4283.22it/s, Materializing param=encoder.layer.4.output.LayerNorm.weight]Loading weights:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 84/103 [00:00<00:00, 4324.50it/s, Materializing param=encoder.layer.4.output.dense.bias]      Loading weights:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 84/103 [00:00<00:00, 4319.62it/s, Materializing param=encoder.layer.4.output.dense.bias]Loading weights:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 85/103 [00:00<00:00, 4361.16it/s, Materializing param=encoder.layer.4.output.dense.weight]Loading weights:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 85/103 [00:00<00:00, 4356.31it/s, Materializing param=encoder.layer.4.output.dense.weight]Loading weights:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 86/103 [00:00<00:00, 4398.74it/s, Materializing param=encoder.layer.5.attention.output.LayerNorm.bias]Loading weights:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 86/103 [00:00<00:00, 4394.24it/s, Materializing param=encoder.layer.5.attention.output.LayerNorm.bias]Loading weights:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 87/103 [00:00<00:00, 4435.34it/s, Materializing param=encoder.layer.5.attention.output.LayerNorm.weight]Loading weights:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 87/103 [00:00<00:00, 4430.60it/s, Materializing param=encoder.layer.5.attention.output.LayerNorm.weight]Loading weights:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 88/103 [00:00<00:00, 4473.54it/s, Materializing param=encoder.layer.5.attention.output.dense.bias]      Loading weights:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 88/103 [00:00<00:00, 4468.78it/s, Materializing param=encoder.layer.5.attention.output.dense.bias]Loading weights:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 89/103 [00:00<00:00, 4511.37it/s, Materializing param=encoder.layer.5.attention.output.dense.weight]Loading weights:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 89/103 [00:00<00:00, 4506.57it/s, Materializing param=encoder.layer.5.attention.output.dense.weight]Loading weights:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 90/103 [00:00<00:00, 4549.14it/s, Materializing param=encoder.layer.5.attention.self.key.bias]      Loading weights:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 90/103 [00:00<00:00, 4544.32it/s, Materializing param=encoder.layer.5.attention.self.key.bias]Loading weights:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 91/103 [00:00<00:00, 4583.22it/s, Materializing param=encoder.layer.5.attention.self.key.weight]Loading weights:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 91/103 [00:00<00:00, 4578.66it/s, Materializing param=encoder.layer.5.attention.self.key.weight]Loading weights:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 92/103 [00:00<00:00, 4621.05it/s, Materializing param=encoder.layer.5.attention.self.query.bias]Loading weights:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 92/103 [00:00<00:00, 4616.40it/s, Materializing param=encoder.layer.5.attention.self.query.bias]Loading weights:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 93/103 [00:00<00:00, 4654.22it/s, Materializing param=encoder.layer.5.attention.self.query.weight]Loading weights:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 93/103 [00:00<00:00, 4647.90it/s, Materializing param=encoder.layer.5.attention.self.query.weight]Loading weights:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 94/103 [00:00<00:00, 4686.87it/s, Materializing param=encoder.layer.5.attention.self.value.bias]  Loading weights:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 94/103 [00:00<00:00, 4681.31it/s, Materializing param=encoder.layer.5.attention.self.value.bias]Loading weights:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 95/103 [00:00<00:00, 4721.19it/s, Materializing param=encoder.layer.5.attention.self.value.weight]Loading weights:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 95/103 [00:00<00:00, 4712.76it/s, Materializing param=encoder.layer.5.attention.self.value.weight]Loading weights:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 96/103 [00:00<00:00, 4752.98it/s, Materializing param=encoder.layer.5.intermediate.dense.bias]    Loading weights:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 96/103 [00:00<00:00, 4748.27it/s, Materializing param=encoder.layer.5.intermediate.dense.bias]Loading weights:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 97/103 [00:00<00:00, 4788.92it/s, Materializing param=encoder.layer.5.intermediate.dense.weight]Loading weights:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 97/103 [00:00<00:00, 4783.96it/s, Materializing param=encoder.layer.5.intermediate.dense.weight]Loading weights:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 98/103 [00:00<00:00, 4824.77it/s, Materializing param=encoder.layer.5.output.LayerNorm.bias]    Loading weights:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 98/103 [00:00<00:00, 4820.19it/s, Materializing param=encoder.layer.5.output.LayerNorm.bias]Loading weights:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 99/103 [00:00<00:00, 4860.83it/s, Materializing param=encoder.layer.5.output.LayerNorm.weight]Loading weights:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 99/103 [00:00<00:00, 4855.82it/s, Materializing param=encoder.layer.5.output.LayerNorm.weight]Loading weights:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 100/103 [00:00<00:00, 4892.80it/s, Materializing param=encoder.layer.5.output.dense.bias]     Loading weights:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 100/103 [00:00<00:00, 4887.55it/s, Materializing param=encoder.layer.5.output.dense.bias]Loading weights:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 101/103 [00:00<00:00, 4924.44it/s, Materializing param=encoder.layer.5.output.dense.weight]Loading weights:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 101/103 [00:00<00:00, 4916.04it/s, Materializing param=encoder.layer.5.output.dense.weight]Loading weights:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 102/103 [00:00<00:00, 4953.39it/s, Materializing param=pooler.dense.bias]                  Loading weights:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 102/103 [00:00<00:00, 4948.57it/s, Materializing param=pooler.dense.bias]Loading weights: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 103/103 [00:00<00:00, 4986.48it/s, Materializing param=pooler.dense.weight]Loading weights: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 103/103 [00:00<00:00, 4980.90it/s, Materializing param=pooler.dense.weight]Loading weights: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 103/103 [00:00<00:00, 4967.67it/s, Materializing param=pooler.dense.weight]
[1mBertModel LOAD REPORT[0m from: sentence-transformers/all-MiniLM-L6-v2
Key                     | Status     |  | 
------------------------+------------+--+-
embeddings.position_ids | UNEXPECTED |  | 

[3mNotes:
- UNEXPECTED[3m	:can be ignored when loading from different task/architecture; not ok if you expect identical arch.[0m
Downloading Tiny Shakespeare...
Total characters: 1115394
First 100 chars: First Citizen:
Before we proceed any further, hear me speak.

All:
Speak, speak.

First Citizen:
You

Chunking with TokenTextSplitter...
  677 chunks, avg length = 1881 chars

Chunking with SemanticSplitterNodeParser (takes a bit)...
  624 chunks, avg length = 1787 chars

Chunking with SentenceWindowNodeParser...
  12453 chunks, avg length = 89 chars

Building indexes...
Done building indexes.

======================================================================
  Technique: Token-Based
  Query: "Who are the two feuding houses?"
======================================================================
  Embedding dim: 384
  First 8 values: [-0.004116630647331476, 0.006742713041603565, -0.016307534649968147, 0.0026377015747129917, -0.05007702857255936, 0.002135436749085784, -0.026932023465633392, -0.0752214789390564]

  Query vector shape: (384,)
  Doc vectors shape: (5, 384)
  Retrieval latency: 19.7 ms

 rank  store_score  cosine_sim  chunk_len                                                                                                                                                          preview
    1       0.3497      0.3497       2058 side?  DERBY: John Duke of Norfolk, Walter Lord Ferrers, Sir Robert Brakenbury, and Sir William Brandon.  RICHMOND: Inter their bodies as becomes their births: 
    2       0.3354      0.3354       1900 would you say ye were beaten out of door; And rail upon the hostess of the house; And say you would present her at the leet, Because she brought stone jugs and 
    3       0.3094      0.3094       2037 is warm and new cut off, Write in the dust this sentence with thy blood, 'Wind-changing Warwick now can change no more.'  WARWICK: O cheerful colours! see where
    4       0.2850      0.2850       1909 done i' the Capitol; who's like to rise, Who thrives and who declines; side factions and give out Conjectural marriages; making parties strong And feebling such
    5       0.2823      0.2823       1736 dog of the house of Montague moves me.  GREGORY: To move is to stir; and to be valiant is to stand: therefore, if thou art moved, thou runn'st away.  SAMPSON: A

======================================================================
  Technique: Semantic
  Query: "Who are the two feuding houses?"
======================================================================
  Embedding dim: 384
  First 8 values: [-0.004116630647331476, 0.006742713041603565, -0.016307534649968147, 0.0026377015747129917, -0.05007702857255936, 0.002135436749085784, -0.026932023465633392, -0.0752214789390564]

  Query vector shape: (384,)
  Doc vectors shape: (5, 384)
  Retrieval latency: 16.6 ms

 rank  store_score  cosine_sim  chunk_len                                                                                                                                                          preview
    1       0.3776      0.3776         30                                                                                                                                   A plague o' both your houses! 
    2       0.2982      0.2982       1522 They say! They'll sit by the fire, and presume to know What's done i' the Capitol; who's like to rise, Who thrives and who declines; side factions and give out 
    3       0.2851      0.2851       1246 Citizen: And you.  CORIOLANUS: Direct me, if it be your will, Where great Aufidius lies: is he in Antium?  Citizen: He is, and feasts the nobles of the state At
    4       0.2795      0.2795         64                                                                                                 Know man from man? dispute his own estate? Lies he not bed-rid? 
    5       0.2787      0.2787        499 Come, sister,--cousin, I would say--pray, pardon me. Go, fellow, get thee home, provide some carts And bring away the armour that is there. Gentlemen, will you 

======================================================================
  Technique: Sentence-Window
  Query: "Who are the two feuding houses?"
======================================================================
  Embedding dim: 384
  First 8 values: [-0.004116630647331476, 0.006742713041603565, -0.016307534649968147, 0.0026377015747129917, -0.05007702857255936, 0.002135436749085784, -0.026932023465633392, -0.0752214789390564]

  Query vector shape: (384,)
  Doc vectors shape: (5, 384)
  Retrieval latency: 146.7 ms

 rank  store_score  cosine_sim  chunk_len                                                                                                                                              preview
    1       0.5126      0.5126         47                                                                                                      here comes two of the house of the Montagues.  
    2       0.4763      0.4763         35                                                                                                                  WARWICK: And I the house of York.  
    3       0.4599      0.4599         41                                                                                                            As I remember, this should be the house. 
    4       0.4565      0.4565         21                                                                                                                                ROMEO: Whose house?  
    5       0.4253      0.4253        148 GLOUCESTER: Two of thy name, both Dukes of Somerset, Have sold their lives unto the house of York; And thou shalt be the third if this sword hold.  

======================================================================
  Technique: Token-Based
  Query: "Who is Romeo in love with?"
======================================================================
  Embedding dim: 384
  First 8 values: [-0.08665773272514343, 0.009958716109395027, 0.06071126088500023, 0.020252786576747894, -0.011710281483829021, 0.043631989508867264, 0.12273851037025452, 0.055907245725393295]

  Query vector shape: (384,)
  Doc vectors shape: (5, 384)
  Retrieval latency: 19.2 ms

 rank  store_score  cosine_sim  chunk_len                                                                                                                                                          preview
    1       0.6003      0.6003       1856 a Montague; The only son of your great enemy.  JULIET: My only love sprung from my only hate! Too early seen unknown, and known too late! Prodigious birth of lo
    2       0.5690      0.5690       1796 new struck nine.  ROMEO: Ay me! sad hours seem long. Was that my father that went hence so fast?  BENVOLIO: It was. What sadness lengthens Romeo's hours?  ROMEO
    3       0.5653      0.5653       1859 you leave me so, you do me wrong.  ROMEO: Tut, I have lost myself; I am not here; This is not Romeo, he's some other where.  BENVOLIO: Tell me in sadness, who i
    4       0.5338      0.5338       1836 I hope, thou wilt be satisfied.  JULIET: Indeed, I never shall be satisfied With Romeo, till I behold him--dead-- Is my poor heart for a kinsman vex'd. Madam, i
    5       0.5261      0.5261       1948 a kinsman to the Montague; Affection makes him false; he speaks not true: Some twenty of them fought in this black strife, And all those twenty could but kill o

======================================================================
  Technique: Semantic
  Query: "Who is Romeo in love with?"
======================================================================
  Embedding dim: 384
  First 8 values: [-0.08665773272514343, 0.009958716109395027, 0.06071126088500023, 0.020252786576747894, -0.011710281483829021, 0.043631989508867264, 0.12273851037025452, 0.055907245725393295]

  Query vector shape: (384,)
  Doc vectors shape: (5, 384)
  Retrieval latency: 13.9 ms

 rank  store_score  cosine_sim  chunk_len                                                                                                                                                          preview
    1       0.6302      0.6302       1104 what's this?  JULIET: A rhyme I learn'd even now Of one I danced withal.  Nurse: Anon, anon! Come, let's away; the strangers all are gone.  Chorus: Now old desi
    2       0.6214      0.6214       1665 But sadly tell me who.  ROMEO: Bid a sick man in sadness make his will: Ah, word ill urged to one that is so ill! In sadness, cousin, I do love a woman.  BENVOL
    3       0.6115      0.6115        605 how sweet is love itself possess'd, When but love's shadows are so rich in joy! News from Verona!--How now, Balthasar! Dost thou not bring me letters from the f
    4       0.5859      0.5859        847 We would as willingly give cure as know.  BENVOLIO: See, where he comes: so please you, step aside; I'll know his grievance, or be much denied.  MONTAGUE: I wou
    5       0.5633      0.5633        223 Well, death's the end of all.  ROMEO: Spakest thou of Juliet? how is it with her? Doth she not think me an old murderer, Now I have stain'd the childhood of our

======================================================================
  Technique: Sentence-Window
  Query: "Who is Romeo in love with?"
======================================================================
  Embedding dim: 384
  First 8 values: [-0.08665773272514343, 0.009958716109395027, 0.06071126088500023, 0.020252786576747894, -0.011710281483829021, 0.043631989508867264, 0.12273851037025452, 0.055907245725393295]

  Query vector shape: (384,)
  Doc vectors shape: (5, 384)
  Retrieval latency: 147.1 ms

 rank  store_score  cosine_sim  chunk_len                                         preview
    1       0.8024      0.8024         17                               ROMEO: Whither?  
    2       0.7949      0.7949         47 ROMEO: Out of her favour, where I am in love.  
    3       0.7853      0.7853         42      ROMEO: Why, such is love's transgression. 
    4       0.7833      0.7833         21                           Where's Romeo's man? 
    5       0.7802      0.7802         22                          ROMEO: Is it even so? 

======================================================================
  Technique: Token-Based
  Query: "Which play contains the line 'To be, or not to be'?"
======================================================================
  Embedding dim: 384
  First 8 values: [-0.009904222562909126, 0.05222729593515396, -0.04228964075446129, -0.0461781769990921, 0.016578052192926407, 0.12208735197782516, 0.08481581509113312, -0.04617413133382797]

  Query vector shape: (384,)
  Doc vectors shape: (5, 384)
  Retrieval latency: 15.7 ms

 rank  store_score  cosine_sim  chunk_len                                                                                                                                                          preview
    1       0.3889      0.3889       1787 you; I'll not be forsworn.  JULIET: Is there no pity sitting in the clouds, That sees into the bottom of my grief? O, sweet my mother, cast me not away! Delay t
    2       0.3813      0.3813       1701 to despair.  JULIET: Saints do not move, though grant for prayers' sake.  ROMEO: Then move not, while my prayer's effect I take. Thus from my lips, by yours, my
    3       0.3755      0.3755       1901 take thee at thy word: Call me but love, and I'll be new baptized; Henceforth I never will be Romeo.  JULIET: What man art thou that thus bescreen'd in night So
    4       0.3723      0.3723       1692 you rat-catcher, will you walk?  TYBALT: What wouldst thou have with me?  MERCUTIO: Good king of cats, nothing but one of your nine lives; that I mean to make b
    5       0.3658      0.3658       1840 shall this speech be spoke for our excuse? Or shall we on without a apology?  BENVOLIO: The date is out of such prolixity: We'll have no Cupid hoodwink'd with a

======================================================================
  Technique: Semantic
  Query: "Which play contains the line 'To be, or not to be'?"
======================================================================
  Embedding dim: 384
  First 8 values: [-0.009904222562909126, 0.05222729593515396, -0.04228964075446129, -0.0461781769990921, 0.016578052192926407, 0.12208735197782516, 0.08481581509113312, -0.04617413133382797]

  Query vector shape: (384,)
  Doc vectors shape: (5, 384)
  Retrieval latency: 13.2 ms

 rank  store_score  cosine_sim  chunk_len                                                                                                                                                          preview
    1       0.4095      0.4095        643 where have you been gadding?  JULIET: Where I have learn'd me to repent the sin Of disobedient opposition To you and your behests, and am enjoin'd By holy Laure
    2       0.3845      0.3845        194 What sayest thou to this tune, matter and method? Is't not drowned i' the last rain, ha? What sayest thou, Trot? Is the world as it was, man? Which is the way? 
    3       0.3672      0.3672       1591 O happy dagger! This is thy sheath; there rust, and let me die.  PAGE: This is the place; there, where the torch doth burn.  First Watchman: The ground is blood
    4       0.3627      0.3627        232 my life is my foe's debt.  BENVOLIO: Away, begone; the sport is at the best.  ROMEO: Ay, so I fear; the more is my unrest.  CAPULET: Nay, gentlemen, prepare not
    5       0.3553      0.3553       1472 For I ne'er saw true beauty till this night.  TYBALT: This, by his voice, should be a Montague. Fetch me my rapier, boy. What dares the slave Come hither, cover

======================================================================
  Technique: Sentence-Window
  Query: "Which play contains the line 'To be, or not to be'?"
======================================================================
  Embedding dim: 384
  First 8 values: [-0.009904222562909126, 0.05222729593515396, -0.04228964075446129, -0.0461781769990921, 0.016578052192926407, 0.12208735197782516, 0.08481581509113312, -0.04617413133382797]

  Query vector shape: (384,)
  Doc vectors shape: (5, 384)
  Retrieval latency: 154.2 ms

 rank  store_score  cosine_sim  chunk_len                                                    preview
    1       0.5407      0.5407         32                           JULIET: What must be shall be.  
    2       0.4852      0.4852         39                    JULIET: Speakest thou from thy heart?  
    3       0.4806      0.4806         48           JULIET: It is, it is: hie hence, be gone, away! 
    4       0.4783      0.4783         58 PERDITA: I see the play so lies That I must bear a part.  
    5       0.4651      0.4651         53      JULIET: What satisfaction canst thou have to-night?  


======================================================================
COMPARISON REPORT
======================================================================

      Technique  Top-1 Cosine  Mean@k Cosine  #Chunks  Avg Chunk Len  Latency (ms)
    Token-Based        0.3497         0.3124      677           1881          19.7
       Semantic        0.3776         0.3038      624           1787          16.6
Sentence-Window        0.5126         0.4661    12453             89         146.7

======================================================================
OBSERVATIONS
======================================================================

Token-based chunking just splits at fixed token boundaries which means
chunks can cut right in the middle of a sentence or scene. This gives
lower cosine scores because the chunks end up mixing content from
different parts of the play. It's fast though and the number of chunks
is predictable.

Semantic chunking uses the embedding model to detect when the topic
shifts and splits there. The chunks are more coherent since each one
usually captures a full scene or thought. For the feuding houses query
the top chunk scored higher because the Montague/Capulet passage wasn't
split across two chunks.

Sentence-window chunking makes tiny chunks (single sentences) but stores
the surrounding sentences in metadata. The cosine scores tend to be
higher because a short, focused sentence matches queries better. But
the context is only in metadata, not in the actual embedding, so it
depends on how you use the results downstream.

======================================================================
CONCLUSION
======================================================================

For the Tiny Shakespeare dataset, semantic chunking works best overall.
It gets the highest top-1 cosine score because chunks line up with
natural topic boundaries in the text. Token-based is simpler and faster
but less accurate. Sentence-window is good for fine-grained matching
where you need the exact sentence but still want context available in
metadata. If I had to pick one for a RAG pipeline on this corpus, I
would go with semantic chunking.

Done.
